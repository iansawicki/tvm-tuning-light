{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"../mmdeploy_model/faster-rcnn-cuda/end2end.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy inputs for faster rcnn\n",
    "dummy_inputs = np.random.randn(1, 3, 800, 1333).astype(np.float32)\n",
    "input_shape = dummy_inputs.shape\n",
    "input_name = model.graph.input[0].name\n",
    "inputs = {input_name: dummy_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to load from type '<class 'onnx.onnx_ml_pb2.ModelProto'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ian/_/_mmdet/nbs/benchmark_onnx.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.79.69.98/home/ian/_/_mmdet/nbs/benchmark_onnx.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run inference with ONNX Runtime\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B100.79.69.98/home/ian/_/_mmdet/nbs/benchmark_onnx.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m session \u001b[39m=\u001b[39m ort\u001b[39m.\u001b[39;49mInferenceSession(model)\n",
      "File \u001b[0;32m~/miniforge3/envs/tiger/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:349\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_bytes \u001b[39m=\u001b[39m path_or_bytes  \u001b[39m# TODO: This is bad as we're holding the memory indefinitely\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to load from type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(path_or_bytes)))\n\u001b[1;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m sess_options\n\u001b[1;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options_initial \u001b[39m=\u001b[39m sess_options\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to load from type '<class 'onnx.onnx_ml_pb2.ModelProto'>'"
     ]
    }
   ],
   "source": [
    "# Run inference with ONNX Runtime\n",
    "session = ort.InferenceSession(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
